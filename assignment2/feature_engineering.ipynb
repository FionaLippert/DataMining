{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dateparser\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold #RepeatedStratifiedKFold, \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data Mining VU data/training_set_VU_DM_2014.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('Data Mining VU data/test_set_VU_DM_2014.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the clean train set\n",
    "#train = pd.read_csv('train_clean.csv')\n",
    "# load the clean test set\n",
    "#test = pd.read_csv('test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trim the upper 2.5% of price_usd and visitor_hist_adr_usd to remove extreme outliers\n",
    "train.price_usd = train.price_usd.apply(lambda x: x if x < 550 else 550)\n",
    "test.price_usd = test.price_usd.apply(lambda x: x if x < 550 else 550)\n",
    "train.visitor_hist_adr_usd = train.visitor_hist_adr_usd.apply(lambda x: x if x < 550 else 550)\n",
    "test.visitor_hist_adr_usd = test.visitor_hist_adr_usd.apply(lambda x: x if x < 550 else 550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Deal with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace missing review score by the worst value 0\n",
    "train.prop_review_score.fillna(0, inplace=True)\n",
    "test.prop_review_score.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mean location_score2 per srch_destination\n",
    "destination_groups_train = train.groupby(['srch_destination_id'])\n",
    "destination_means_train = destination_groups_train.apply(lambda x: x.prop_location_score2.mean())\n",
    "destination_groups_test = test.groupby(['srch_destination_id'])\n",
    "destination_means_test = destination_groups_test.apply(lambda x: x.prop_location_score2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mean location_score2 per prop_country\n",
    "country_groups_train = train.groupby(['prop_country_id'])\n",
    "country_means_train = country_groups_train.apply(lambda x: x.prop_location_score2.mean())\n",
    "country_groups_test = test.groupby(['prop_country_id'])\n",
    "country_means_test = country_groups_test.apply(lambda x: x.prop_location_score2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace missing location_score2 by average within the destination_id cluster\n",
    "destination_mean_loc_score2_train = [destination_means_train[dest] for dest in train.srch_destination_id]\n",
    "train.prop_location_score2.fillna(dict(zip(train.index.values, destination_mean_loc_score2_train)), inplace=True)\n",
    "destination_mean_loc_score2_test = [destination_means_test[dest] for dest in test.srch_destination_id]\n",
    "test.prop_location_score2.fillna(dict(zip(test.index.values, destination_mean_loc_score2_test)), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace missing location_score2 by average within the prop_country_id cluster\n",
    "country_mean_loc_score2_train = [country_means_train[country] for country in train.prop_country_id]\n",
    "train.prop_location_score2.fillna(dict(zip(train.index.values, country_mean_loc_score2_train)), inplace=True)\n",
    "country_mean_loc_score2_test = [country_means_test[country] for country in test.prop_country_id]\n",
    "test.prop_location_score2.fillna(dict(zip(test.index.values, country_mean_loc_score2_test)), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace remaining missing location_score by overall average\n",
    "loc_score2_mean_train = train.prop_location_score2.mean()\n",
    "train.prop_location_score2.fillna(loc_score2_mean_train, inplace=True)\n",
    "loc_score2_mean_test = test.prop_location_score2.mean()\n",
    "test.prop_location_score2.fillna(loc_score2_mean_test, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace missing srch_query_affinity_score by minimum value (hotels that did not register in any internet searches are punished)\n",
    "train.srch_query_affinity_score.replace(0,np.nan, inplace=True)\n",
    "test.srch_query_affinity_score.replace(0,np.nan, inplace=True)\n",
    "sqas_min = min(train.srch_query_affinity_score.min(),test.srch_query_affinity_score.min())\n",
    "train.srch_query_affinity_score.fillna(sqas_min, inplace=True)\n",
    "test.srch_query_affinity_score.fillna(sqas_min, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace missing orig_destination_distance by average\n",
    "train.orig_destination_distance.fillna(train.orig_destination_distance.mean(), inplace=True)\n",
    "test.orig_destination_distance.fillna(test.orig_destination_distance.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mean orig_destination_distance per (srch_destination_id, visitor_location_country_id) cluster\n",
    "orig_destination_groups_train = train.groupby(['srch_destination_id', 'visitor_location_country_id'])\n",
    "distance_means_train = orig_destination_groups_train.apply(lambda x: x.orig_destination_distance.mean())\n",
    "orig_destination_groups_test = test.groupby(['srch_destination_id', 'visitor_location_country_id'])\n",
    "distance_means_test = orig_destination_groups_test.apply(lambda x: x.orig_destination_distance.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mean orig_destination_distance per (srch_destination_id, site_id) cluster\n",
    "site_destination_groups_train = train.groupby(['srch_destination_id', 'site_id'])\n",
    "distance_means2_train = site_destination_groups_train.apply(lambda x: x.orig_destination_distance.mean())\n",
    "site_destination_groups_test = test.groupby(['srch_destination_id', 'site_id'])\n",
    "distance_means2_test = site_destination_groups_test.apply(lambda x: x.orig_destination_distance.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace missing orig_destination_distance by average within the (destination_id, visitor_location_country_id) cluster\n",
    "orig_destination_distance_estimate_train = [distance_means_train[(dest, orig)] for dest, orig in zip(train.srch_destination_id, train.visitor_location_country_id)]\n",
    "train.orig_destination_distance.fillna(dict(zip(train.index.values, orig_destination_distance_estimate_train)), inplace=True)\n",
    "orig_destination_distance_estimate_test = [distance_means_test[(dest, orig)] for dest, orig in zip(test.srch_destination_id, test.visitor_location_country_id)]\n",
    "test.orig_destination_distance.fillna(dict(zip(test.index.values, orig_destination_distance_estimate_test)), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace missing orig_destination_distance by average within the (destination_id, site_id) cluster\n",
    "orig_destination_distance_estimate2_train = [distance_means2_train[(dest, site)] for dest, site in zip(train.srch_destination_id, train.site_id)]\n",
    "train.orig_destination_distance.fillna(dict(zip(train.index.values, orig_destination_distance_estimate2_train)), inplace=True)\n",
    "orig_destination_distance_estimate2_test = [distance_means2_test[(dest, site)] for dest, site in zip(test.srch_destination_id, test.site_id)]\n",
    "test.orig_destination_distance.fillna(dict(zip(test.index.values, orig_destination_distance_estimate2_test)), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 60)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean hist_starrating per site_id\n",
    "hist_starrating_groups_train = train.groupby(['site_id'])\n",
    "starrating_means_train = hist_starrating_groups_train.apply(lambda x: x.visitor_hist_starrating.mean())\n",
    "hist_starrating_groups_test = test.groupby(['site_id'])\n",
    "starrating_means_test = hist_starrating_groups_test.apply(lambda x: x.visitor_hist_starrating.mean())\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 61)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace missing hist_starrating by average within the site_id cluster\n",
    "site_mean_hist_starrating_train = [starrating_means_train[site] for site in train.site_id]\n",
    "train['visitor_hist_starrating_filled'] = train.visitor_hist_starrating.copy()\n",
    "train.visitor_hist_starrating_filled.fillna(dict(zip(train.index.values, site_mean_hist_starrating_train)), inplace=True)\n",
    "train.visitor_hist_starrating_filled.fillna(train.visitor_hist_starrating.mean(), inplace=True)\n",
    "site_mean_hist_starrating_test = [starrating_means_test[site] for site in test.site_id]\n",
    "test['visitor_hist_starrating_filled'] = test.visitor_hist_starrating.copy()\n",
    "test.visitor_hist_starrating_filled.fillna(dict(zip(test.index.values, site_mean_hist_starrating_test)), inplace=True)\n",
    "test.visitor_hist_starrating_filled.fillna(test.visitor_hist_starrating.mean(), inplace=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 63)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean hist_adr_usd per site_id\n",
    "hist_price_groups_train = train.groupby(['site_id'])\n",
    "hist_price_means_train = hist_price_groups_train.apply(lambda x: x.visitor_hist_adr_usd.mean())\n",
    "hist_price_groups_test = test.groupby(['site_id'])\n",
    "hist_price_means_test = hist_price_groups_test.apply(lambda x: x.visitor_hist_adr_usd.mean())\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace missing hist_adr_usd by average within the site_id cluster\n",
    "site_mean_hist_price_train = [hist_price_means_train[site] for site in train.site_id]\n",
    "train['visitor_hist_adr_usd_filled'] = train.visitor_hist_adr_usd.copy()\n",
    "train.visitor_hist_adr_usd_filled.fillna(dict(zip(train.index.values, site_mean_hist_price_train)), inplace=True)\n",
    "train.visitor_hist_adr_usd_filled.fillna(train.visitor_hist_adr_usd.mean(), inplace=True)\n",
    "site_mean_hist_price_test = [hist_price_means_test[site] for site in test.site_id]\n",
    "test['visitor_hist_adr_usd_filled'] = test.visitor_hist_adr_usd.copy()\n",
    "test.visitor_hist_adr_usd_filled.fillna(dict(zip(test.index.values, site_mean_hist_price_test)), inplace=True)\n",
    "test.visitor_hist_adr_usd_filled.fillna(test.visitor_hist_adr_usd.mean(), inplace=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 56)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape\n",
    "train['srch_query_affinity_bins'], affinity_bins = pd.cut(train.srch_query_affinity_score.apply(lambda x: x if x >= -100 else -100), 5, labels=np.arange(5), retbins=True)\n",
    "#test['srch_query_affinity_bins'] = pd.cut(test.srch_query_affinity_score.apply(lambda x: x if x >= -100 else -100), bins=affinity_bins, labels=np.arange(5))\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 58)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape\n",
    "train['star_diff'] = np.abs(train.visitor_hist_starrating - train.prop_starrating)\n",
    "train['star_diff_bool'] = train.star_diff.apply(lambda x: 1 if x <= 1 else 0) # 1 means match, 0 mismatch\n",
    "test['star_diff'] = np.abs(test.visitor_hist_starrating - test.prop_starrating)\n",
    "test['star_diff_bool'] = test.star_diff.apply(lambda x: 1 if x <= 1 else 0) # 1 means match, 0 mismatch\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 60)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape\n",
    "train['price_diff'] = np.abs(train.visitor_hist_adr_usd - train.price_usd)\n",
    "train['price_diff_bool'] = train.price_diff.apply(lambda x: 1 if x <= 20 else 0) # 1 means match, 0 mismatch (use 25% quantil as threshold)\n",
    "test['price_diff'] = np.abs(test.visitor_hist_adr_usd - test.price_usd)\n",
    "test['price_diff_bool'] = test.price_diff.apply(lambda x: 1 if x <= 20 else 0) # 1 means match, 0 mismatch (use 25% quantil as threshold)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 62)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['star_diff_filled'] = np.abs(train.visitor_hist_starrating_filled - train.prop_starrating)\n",
    "test['star_diff_filled'] = np.abs(test.visitor_hist_starrating_filled - test.prop_starrating)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 63)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['star_diff_bins'], star_diff_bins = pd.cut(train['star_diff_filled'], 4, labels=np.arange(4), retbins=True)\n",
    "test['star_diff_bins'] = pd.cut(test['star_diff_filled'], bins=star_diff_bins, labels=np.arange(4))\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 65)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['price_diff_filled'] = np.abs(train.visitor_hist_adr_usd_filled - train.price_usd)\n",
    "test['price_diff_filled'] = np.abs(test.visitor_hist_adr_usd_filled - test.price_usd)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 66)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['price_diff_bins'] = pd.qcut(train['price_diff_filled'], 10, labels=np.arange(10))\n",
    "test['price_diff_bins'] = pd.qcut(test['price_diff_filled'], 10 , labels=np.arange(10))\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 67)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['month'] = train.date_time.apply(lambda x: x.split('-')[1])\n",
    "test['month'] = test.date_time.apply(lambda x: x.split('-')[1])\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 67)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# difference between mean prices of booked hotels and mean prices of not-booked hotels per site_id\n",
    "site_groups = train.groupby(['site_id'])\n",
    "price_diff = site_groups.apply(lambda x: x[x.booking_bool == 1].price_usd.mean() - x[x.booking_bool == 0].price_usd.mean())\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 68)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['willingness_to_pay'] = train.site_id.apply(lambda x: 1 if price_diff[x] > 0 else 0)\n",
    "test['willingness_to_pay'] = test.site_id.apply(lambda x: 1 if price_diff[x] > 0 else 0)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 68)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotel_groups = train.groupby(['prop_id'])\n",
    "hotel_quality = hotel_groups.apply(lambda x: float(x.booking_bool.sum())/x.booking_bool.values.size)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 69)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['prop_desirability'] = train.prop_id.apply(lambda x: hotel_quality[x])\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hotel_quality(prop_id):\n",
    "    try:\n",
    "        return hotel_quality[prop_id]\n",
    "    except:\n",
    "        return hotel_quality.mean()\n",
    "    \n",
    "test['prop_desirability'] = test.prop_id.apply(get_hotel_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 70)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_prop_ids = train.prop_id.unique()\n",
    "leave_out = np.random.choice(all_prop_ids, int(0.06*len(all_prop_ids)), replace=False)\n",
    "hotel_quality.loc[leave_out] = hotel_quality.mean()\n",
    "train['prop_desirability_incomplete'] = train.prop_id.apply(lambda x: hotel_quality[x])\n",
    "train.shape\n",
    "#test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 71)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the two location scores into one joint feature\n",
    "train['prop_location_score_combined'] = (train.prop_location_score1 / train.prop_location_score1.mean()) - np.log(train.prop_location_score2 + 0.00001)/np.mean(np.log(train.prop_location_score2 + 0.000001))\n",
    "test['prop_location_score_combined'] = (test.prop_location_score1 / test.prop_location_score1.mean()) - np.log(test.prop_location_score2 + 0.00001)/np.mean(np.log(test.prop_location_score2 + 0.000001))\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 72)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['booked_clicked_combined'] = train.booking_bool + train.click_bool\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 54)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_feature(df, feature_name, normalize_wrt_feature):\n",
    "    groups = df.groupby(normalize_wrt_feature)\n",
    "    avg_feature_vals = groups.apply(lambda x: x[feature_name].mean())\n",
    "    new_col = []\n",
    "    for row in df.itertuples():\n",
    "        normed_val = getattr(row, feature_name) / avg_feature_vals[getattr(row, normalize_wrt_feature)]\n",
    "        new_col.append(normed_val if np.isfinite(normed_val) else 0)\n",
    "    df[feature_name + '_norm_' + normalize_wrt_feature] = new_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tamika/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# normalize numerical features with regard to 'srch_destination_id'\n",
    "normalize_feature(train, 'price_usd','srch_destination_id')\n",
    "normalize_feature(test, 'price_usd','srch_destination_id')\n",
    "normalize_feature(train, 'prop_location_score1','srch_destination_id')\n",
    "normalize_feature(test, 'prop_location_score1','srch_destination_id')\n",
    "normalize_feature(train, 'prop_location_score2','srch_destination_id')\n",
    "normalize_feature(test, 'prop_location_score2','srch_destination_id')\n",
    "normalize_feature(train, 'prop_review_score','srch_destination_id')\n",
    "normalize_feature(test, 'prop_review_score','srch_destination_id')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize numerical features with regard to 'srch_id'\n",
    "normalize_feature(train, 'price_usd','srch_id')\n",
    "#normalize_feature(test, 'price_usd','srch_id')\n",
    "normalize_feature(train, 'prop_location_score1','srch_id')\n",
    "#normalize_feature(test, 'prop_location_score1','srch_id')\n",
    "normalize_feature(train, 'prop_location_score2','srch_id')\n",
    "#normalize_feature(test, 'prop_location_score2','srch_id')\n",
    "normalize_feature(train, 'prop_review_score','srch_id')\n",
    "#normalize_feature(test, 'prop_review_score','srch_id')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize numerical features with regard to 'site_id'\n",
    "normalize_feature(train, 'price_usd','site_id')\n",
    "#normalize_feature(test, 'price_usd','site_id')\n",
    "normalize_feature(train, 'prop_location_score1','site_id')\n",
    "#normalize_feature(test, 'prop_location_score1','site_id')\n",
    "normalize_feature(train, 'prop_location_score2','site_id')\n",
    "#normalize_feature(test, 'prop_location_score2','site_id')\n",
    "normalize_feature(train, 'prop_review_score','site_id')\n",
    "#normalize_feature(test, 'prop_review_score','site_id')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average hotel features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute average characteristics per prop_id\n",
    "properties_train = train[['price_usd','prop_location_score1','prop_location_score2','prop_location_score_combined','prop_review_score','prop_starrating','prop_id']].groupby(['prop_id'])\n",
    "avg_price_usd_per_prop_train = properties_train.apply(lambda x: x.price_usd.mean())\n",
    "avg_loc_score1_per_prop_train = properties_train.apply(lambda x: x.prop_location_score1.mean())\n",
    "avg_loc_score2_per_prop_train = properties_train.apply(lambda x: x.prop_location_score2.mean())\n",
    "avg_loc_score_combined_per_prop_train = properties_train.apply(lambda x: x.prop_location_score_combined.mean())\n",
    "avg_review_score_per_prop_train = properties_train.apply(lambda x: x.prop_review_score.mean())\n",
    "avg_starrating_per_prop_train = properties_train.apply(lambda x: x.prop_starrating.mean())\n",
    "\n",
    "# create features\n",
    "train['avg_prop_price_usd'] = train.prop_id.apply(lambda x: avg_price_usd_per_prop_train[x])\n",
    "train['avg_prop_location_score1'] = train.prop_id.apply(lambda x: avg_loc_score1_per_prop_train[x])\n",
    "train['avg_prop_location_score2'] = train.prop_id.apply(lambda x: avg_loc_score2_per_prop_train[x])\n",
    "train['avg_prop_location_score_combined'] = train.prop_id.apply(lambda x: avg_loc_score_combined_per_prop_train[x])\n",
    "train['avg_prop_review_score'] = train.prop_id.apply(lambda x: avg_review_score_per_prop_train[x])\n",
    "train['avg_prop_starrating'] = train.prop_id.apply(lambda x: avg_starrating_per_prop_train[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute average characteristics per prop_id\n",
    "properties_test = test[['price_usd','prop_location_score1','prop_location_score2','prop_location_score_combined','prop_review_score','prop_starrating','prop_id']].groupby(['prop_id'])\n",
    "avg_price_usd_per_prop_test = properties_test.apply(lambda x: x.price_usd.mean())\n",
    "avg_loc_score1_per_prop_test = properties_test.apply(lambda x: x.prop_location_score1.mean())\n",
    "avg_loc_score2_per_prop_test = properties_test.apply(lambda x: x.prop_location_score2.mean())\n",
    "avg_loc_score_combined_per_prop_test = properties_test.apply(lambda x: x.prop_location_score_combined.mean())\n",
    "avg_review_score_per_prop_test = properties_test.apply(lambda x: x.prop_review_score.mean())\n",
    "avg_starrating_per_prop_test = properties_test.apply(lambda x: x.prop_starrating.mean())\n",
    "\n",
    "# create features\n",
    "test['avg_prop_price_usd'] = test.prop_id.apply(lambda x: avg_price_usd_per_prop_test[x])\n",
    "test['avg_prop_location_score1'] = test.prop_id.apply(lambda x: avg_loc_score1_per_prop_test[x])\n",
    "test['avg_prop_location_score2'] = test.prop_id.apply(lambda x: avg_loc_score2_per_prop_test[x])\n",
    "test['avg_prop_location_score_combined'] = test.prop_id.apply(lambda x: avg_loc_score_combined_per_prop_test[x])\n",
    "test['avg_prop_review_score'] = test.prop_id.apply(lambda x: avg_review_score_per_prop_test[x])\n",
    "test['avg_prop_starrating'] = test.prop_id.apply(lambda x: avg_starrating_per_prop_test[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete features that we decided not to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 48)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.drop(['comp1_rate','comp1_inv','comp1_rate_percent_diff','comp2_rate','comp2_inv','comp2_rate_percent_diff','comp3_rate','comp3_inv','comp3_rate_percent_diff','comp4_rate','comp4_inv','comp4_rate_percent_diff','comp5_rate','comp5_inv','comp5_rate_percent_diff','comp6_rate','comp6_inv','comp6_rate_percent_diff','comp7_rate','comp7_inv','comp7_rate_percent_diff','comp8_rate','comp8_inv','comp8_rate_percent_diff'], axis=1, inplace=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a model for position estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 48)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create samples that always contain all queries of a srch_id\n",
    "srch_id_groups = train.groupby('srch_id')\n",
    "all_srch_ids = train.srch_id.unique()\n",
    "num_samples = int(0.25*len(all_srch_ids)) # 25% of the entire train set\n",
    "np.random.seed(0)\n",
    "position_model_ids = np.random.choice(all_srch_ids, num_samples, replace=False)\n",
    "position_model_sample = pd.concat([srch_id_groups.get_group(group) for group in position_model_ids])\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958347, 48)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_features = ['site_id','prop_country_id','prop_starrating','prop_review_score','prop_brand_bool','prop_location_score2','price_usd','promotion_flag','srch_children_count','srch_room_count','visitor_hist_starrating_filled','star_diff_filled','prop_review_score_norm_srch_destination_id','price_usd_norm_srch_id','price_diff_bins','price_usd_norm_srch_destination_id','prop_location_score1_norm_srch_destination_id','prop_location_score2_norm_srch_destination_id','prop_review_score_norm_srch_id','prop_location_score1_norm_srch_id','prop_location_score2_norm_srch_id','prop_location_score_combined','price_usd_norm_site_id','prop_location_score2_norm_site_id','prop_review_score_norm_site_id','avg_prop_price_usd','avg_prop_location_score2','avg_prop_location_score_combined','avg_prop_review_score','avg_prop_starrating']\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['prop_review_score_norm_srch_destination_id' 'price_usd_norm_srch_id'\\n 'prop_location_score1_norm_srch_destination_id'\\n 'prop_location_score2_norm_srch_destination_id'\\n 'prop_review_score_norm_srch_id' 'prop_location_score1_norm_srch_id'\\n 'prop_location_score2_norm_srch_id' 'price_usd_norm_site_id'\\n 'prop_location_score2_norm_site_id' 'prop_review_score_norm_site_id'\\n 'avg_prop_price_usd' 'avg_prop_location_score2'\\n 'avg_prop_location_score_combined' 'avg_prop_review_score'\\n 'avg_prop_starrating'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-feccf41474f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# evaluate performance of position model with cross-validation on the entire training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'position'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tamika/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tamika/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2175\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tamika/anaconda/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1269\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['prop_review_score_norm_srch_destination_id' 'price_usd_norm_srch_id'\\n 'prop_location_score1_norm_srch_destination_id'\\n 'prop_location_score2_norm_srch_destination_id'\\n 'prop_review_score_norm_srch_id' 'prop_location_score1_norm_srch_id'\\n 'prop_location_score2_norm_srch_id' 'price_usd_norm_site_id'\\n 'prop_location_score2_norm_site_id' 'prop_review_score_norm_site_id'\\n 'avg_prop_price_usd' 'avg_prop_location_score2'\\n 'avg_prop_location_score_combined' 'avg_prop_review_score'\\n 'avg_prop_starrating'] not in index\""
     ]
    }
   ],
   "source": [
    "# evaluate performance of position model with cross-validation on the entire training set\n",
    "cv = StratifiedKFold(n_splits=4, random_state=0)\n",
    "scores = cross_val_score(RandomForestRegressor(n_estimators=50), train[pos_features], train['position'], cv=cv)\n",
    "print(np.mean(scores))\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the position model\n",
    "clf = RandomForestRegressor(n_estimators=50, random_state=0)\n",
    "clf.fit(train_sample[pos_features], position_model_sample['position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the trained model to file\n",
    "pickle.dump(clf, open('position_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the position model from file\n",
    "#clf = pickle.load(open('position_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['position_estimate'] = clf.predict(train[pos_features]).astype(int)\n",
    "test['position_estimate'] = clf.predict(test[pos_features]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>prop_location_score2_norm_srch_id</td>\n",
       "      <td>0.108617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>prop_review_score_norm_srch_id</td>\n",
       "      <td>0.091706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>prop_location_score1_norm_srch_id</td>\n",
       "      <td>0.090787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prop_location_score2</td>\n",
       "      <td>0.086432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>prop_location_score2_norm_srch_destination_id</td>\n",
       "      <td>0.068569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>price_usd_norm_srch_destination_id</td>\n",
       "      <td>0.067228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>price_usd</td>\n",
       "      <td>0.060701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>prop_location_score1</td>\n",
       "      <td>0.054462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>avg_prop_location_score_combined</td>\n",
       "      <td>0.051149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>month</td>\n",
       "      <td>0.040025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>avg_prop_price_usd</td>\n",
       "      <td>0.039470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>prop_id</td>\n",
       "      <td>0.038692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>srch_destination_id</td>\n",
       "      <td>0.034227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>prop_review_score_norm_srch_destination_id</td>\n",
       "      <td>0.033318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>prop_location_score1_norm_srch_destination_id</td>\n",
       "      <td>0.032609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>site_id</td>\n",
       "      <td>0.023984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>srch_children_count</td>\n",
       "      <td>0.016349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prop_country_id</td>\n",
       "      <td>0.012906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>promotion_flag</td>\n",
       "      <td>0.010284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>avg_prop_starrating</td>\n",
       "      <td>0.009361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prop_starrating</td>\n",
       "      <td>0.009102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>srch_room_count</td>\n",
       "      <td>0.007113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prop_brand_bool</td>\n",
       "      <td>0.005176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prop_review_score</td>\n",
       "      <td>0.003926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>avg_prop_review_score</td>\n",
       "      <td>0.003807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          feature  feature_importance\n",
       "20              prop_location_score2_norm_srch_id            0.108617\n",
       "18                 prop_review_score_norm_srch_id            0.091706\n",
       "19              prop_location_score1_norm_srch_id            0.090787\n",
       "6                            prop_location_score2            0.086432\n",
       "17  prop_location_score2_norm_srch_destination_id            0.068569\n",
       "15             price_usd_norm_srch_destination_id            0.067228\n",
       "7                                       price_usd            0.060701\n",
       "5                            prop_location_score1            0.054462\n",
       "24               avg_prop_location_score_combined            0.051149\n",
       "12                                          month            0.040025\n",
       "21                             avg_prop_price_usd            0.039470\n",
       "13                                        prop_id            0.038692\n",
       "11                            srch_destination_id            0.034227\n",
       "14     prop_review_score_norm_srch_destination_id            0.033318\n",
       "16  prop_location_score1_norm_srch_destination_id            0.032609\n",
       "0                                         site_id            0.023984\n",
       "9                             srch_children_count            0.016349\n",
       "2                                 prop_country_id            0.012906\n",
       "10                                 promotion_flag            0.010284\n",
       "23                            avg_prop_starrating            0.009361\n",
       "3                                 prop_starrating            0.009102\n",
       "8                                 srch_room_count            0.007113\n",
       "1                                 prop_brand_bool            0.005176\n",
       "4                               prop_review_score            0.003926\n",
       "22                          avg_prop_review_score            0.003807"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'feature':pos_features, 'feature_importance':clf.feature_importances_}).sort_values(by='feature_importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete columns that cannot be used in the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete unnecessary columns\n",
    "train.drop(columns=['date_time', 'visitor_hist_starrating', 'visitor_hist_adr_usd', 'gross_bookings_usd', 'star_diff', 'price_diff'], inplace=True)\n",
    "test.drop(columns=['date_time', 'visitor_hist_starrating', 'visitor_hist_adr_usd', 'star_diff', 'price_diff'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the cleaned datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('train_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.to_csv('test_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use th following code to divide the training set into samples. Apply all cross-validations on xval_sample or subsets of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# divide the training set into subsamples for: feature engineering (position modeling), cross-validation, single training, single validation\n",
    "srch_id_groups = train.groupby('srch_id')\n",
    "all_srch_ids = train.srch_id.unique()\n",
    "num_samples_pos = int(0.25*len(all_srch_ids)) # 25%\n",
    "num_samples_train = int(0.5*len(all_srch_ids)) # 50%\n",
    "np.random.seed(0)\n",
    "position_model_ids = np.random.choice(all_srch_ids, num_samples_pos, replace=False)\n",
    "xval_ids = list(set(all_srch_ids)-set(position_model_ids))\n",
    "training_ids = np.random.choice(xval_ids, num_samples_train, replace=False)\n",
    "validation_ids = list(set(xval_ids)-set(training_ids))\n",
    "\n",
    "xval_sample = train[train.srch_id.isin(xval_ids)]\n",
    "train_sample = train[train.srch_id.isin(training_ids)]\n",
    "validation_sample = train[train.srch_id.isin(validation_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute this before making the final predictions for the test set !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['price_diff_bins'] = pd.qcut(test['price_diff_filled'], 10 , labels=np.arange(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
