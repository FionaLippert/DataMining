{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "from ndcg_calc import ndcg_calc\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig = pd.read_csv('processed_data_final/train_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the training set into subsamples for: feature engineering (position modeling), cross-validation, single training, single validation\n",
    "\n",
    "all_srch_ids = train_orig.srch_id.unique()\n",
    "num_samples_pos = int(0.25*len(all_srch_ids)) # 25%\n",
    "num_samples_train = int(0.5*len(all_srch_ids)) # 50%\n",
    "np.random.seed(0)\n",
    "position_model_ids = np.random.choice(all_srch_ids, num_samples_pos, replace=False)\n",
    "xval_ids = list(set(all_srch_ids)-set(position_model_ids))\n",
    "training_ids = np.random.choice(xval_ids, num_samples_train, replace=False)\n",
    "validation_ids = list(set(xval_ids)-set(training_ids))\n",
    "\n",
    "# reducing the size of the cross-validation set\n",
    "xval_ids_train = np.random.choice(xval_ids, int(len(xval_ids)*0.5), replace=False)\n",
    "\n",
    "# untouched sample is left for extra validation and also because my memory cannot cope\n",
    "xval_sample = train_orig[train_orig.srch_id.isin(xval_ids_train)]\n",
    "untouched_sample = train_orig[~train_orig.srch_id.isin(xval_ids_train)]\n",
    "#train_sample = train_orig[train_orig.srch_id.isin(training_ids)]\n",
    "#validation_sample = train_orig[train_orig.srch_id.isin(validation_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "del(train_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# columns to drop in general\n",
    "cols_to_drop1 = ['booking_bool','click_bool','prop_desirability','position']\n",
    "\n",
    "# columns to drop with features that are not important enough, needed to drop it to avoid memory errors\n",
    "\n",
    "cols_to_drop2 = ['willingness_to_pay', 'avg_prop_starrating', 'price_diff_bins','srch_query_affinity_bins',\n",
    "                 'star_diff_bins','avg_prop_review_score', 'price_diff_bool']\n",
    "\n",
    "#drop columns\n",
    "xval_sample.drop(cols_to_drop1, axis=1, inplace = True)\n",
    "xval_sample.drop(cols_to_drop2, axis=1, inplace = True)\n",
    "untouched_sample.drop(['prop_desirability','position'], axis=1, inplace = True)\n",
    "#train_sample.drop(cols_to_drop, axis=1, inplace = True)\n",
    "#validation_sample.drop(cols_to_drop, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract labels\n",
    "xval_sample.sort_values(by='srch_id', inplace=True)\n",
    "labels = xval_sample.booked_clicked_combined\n",
    "xval_sample.drop(['booked_clicked_combined'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xval_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to break srch_id's into folds \n",
    "def get_fold_ids(unique_srch_ids, num_folds=5):\n",
    "    fraction = 1/num_folds\n",
    "    final_fold_ids = dict()\n",
    "    for i in range(num_folds):\n",
    "        num_samples = int(fraction*len(unique_srch_ids))\n",
    "        train_sample_ids = np.random.choice(unique_srch_ids, num_samples, replace=False)\n",
    "        unique_srch_ids = list(set(unique_srch_ids)-set(train_sample_ids))\n",
    "        set_key = 'id_set' + str(i)\n",
    "        final_fold_ids[set_key] = train_sample_ids\n",
    "    return final_fold_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide ids into n subsets of equal size\n",
    "num_folds = 3 # number of folds for cross-validation\n",
    "fold_ids = get_fold_ids(xval_sample.srch_id.unique(),num_folds)\n",
    "keys = list(fold_ids.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# parameter_range sets the values for parameter sweep\n",
    "num_iter_range = [2000, 1500, 1000, 500]\n",
    "learning_rate_range = [0.3, 0.2, 0.1, 0.05]\n",
    "\n",
    "\n",
    "scores = np.zeros((len(num_iter_range),len(learning_rate_range),num_folds))\n",
    "models = np.zeros((len(num_iter_range),len(learning_rate_range),num_folds))\n",
    "\n",
    "for i,num_iter in enumerate(num_iter_range):\n",
    "    for j, rate in enumerate(learning_rate_range):\n",
    "        for k in range(num_folds):\n",
    "            params = {'task': 'train',\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'lambdarank',\n",
    "                    'metric': 'ndcg',\n",
    "                    'label_gain' : {0,1,5},\n",
    "                    'ndcg_at': 38,\n",
    "                    'num_leaves':10,\n",
    "                    'feature_fraction': 0.9,\n",
    "                    'bagging_fraction': 0.8,\n",
    "                    'bagging_freq': 1,\n",
    "                    'verbose': 0}\n",
    "            params['num_iterations'] = num_iter\n",
    "            params['learning_rate'] = rate\n",
    "\n",
    "            # divide the srch_ids into training and validation set\n",
    "            id_valid  = k # validation set index\n",
    "            valid_set_ids = fold_ids[keys[id_valid]] # get validation set ids\n",
    "\n",
    "            '''\n",
    "            if k == num_folds-1:\n",
    "                train_set_ids = fold_ids[keys[0]]\n",
    "            else:\n",
    "                train_set_ids = fold_ids[keys[id_valid+1]]\n",
    "            '''\n",
    "            #get subsets of the full train set for training and validation sets\n",
    "            valid_set = xval_sample[xval_sample.srch_id.isin(valid_set_ids)]\n",
    "            train_set = xval_sample[~xval_sample.srch_id.isin(valid_set_ids)]\n",
    "            \n",
    "            training_set_size.append(len(train_set))\n",
    "\n",
    "            #prepare the label \n",
    "            train_label = labels[train_set.index]\n",
    "            valid_label = labels[valid_set.index]\n",
    "\n",
    "            #get group data\n",
    "            train_group = train_set.groupby(['srch_id']).size().values\n",
    "            valid_group = valid_set.groupby(['srch_id']).size().values\n",
    "\n",
    "            # put into lightGBM format\n",
    "            lgb_train = lgb.Dataset(train_set,label=train_label, group=train_group)\n",
    "            lgb_val = lgb.Dataset(valid_set, label=valid_label, group=valid_group)\n",
    "            \n",
    "            #free memory\n",
    "            del(train_set)\n",
    "            del(valid_set)\n",
    "            \n",
    "            # train the lambdarank model with given parameters\n",
    "            lgbm_model = lgb.train(params, train_set = lgb_train, valid_sets = lgb_val,\n",
    "                                   verbose_eval=200)\n",
    "            models[i,j,k] = lgbm_model\n",
    "            scores[i,j,k] = lgbm_model.best_score['valid_0']['ndcg@38']\n",
    "                    \n",
    "            # free memory\n",
    "            del(lgb_train)\n",
    "            del(lgb_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(scores)\n",
    "mean_scores = np.mean(scores, axis=2)\n",
    "std_scores = np.std(scores, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_scores)\n",
    "print(std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(mean_scores, aspect='auto', cmap=plt.cm.coolwarm)\n",
    "plt.xticks(range(len(learning_rate_range)),learning_rate_range)\n",
    "plt.yticks(range(len(num_iter_range)), num_iter_range)\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('num_iterations')\n",
    "plt.title('hyperparameter grid search')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('nDCG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x_str = 'Num_iterations'\n",
    "plt.figure()\n",
    "plt.plot(parameter_range, mean_scores,marker='*',linestyle='--')\n",
    "plt.xlabel(x_str)\n",
    "plt.ylabel('nDCG')\n",
    "plt.title('hyperparameter tuning for lambdarank')\n",
    "plt.grid(color='gray',alpha=0.3, linestyle='--')\n",
    "#plt.xticks(np.arange(min(parameter_range), max(parameter_range)+100))\n",
    "#plt.savefig('output/num_iter_vs_ndcg_train.pdf',bbox_inches='tight')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some extra validation on the untouched portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra check on the portion of the set that was not used in cross-validation at all\n",
    "test_scores = np.zeros(len(models))\n",
    "for i,model in enumerate(models):\n",
    "    prediction = model.predict(untouched_sample)\n",
    "    test_score = ndcg_calc(untouched_sample[['srch_id','click_bool','booking_bool']], prediction)\n",
    "    test_scores[i] = test_score\n",
    "    print('Model {0:d} score is {1:.8f}'.format(i, test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = test_scores.reshape((len(num_iter_range),len(learning_rate_range),num_folds))\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test_scores = np.mean(test_scores, axis=2)\n",
    "mean_test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(mean_scores, aspect='auto', cmap=plt.cm.viridis_r)\n",
    "plt.xticks(range(len(learning_rate_range)),learning_rate_range)\n",
    "plt.yticks(range(len(num_iter_range)), num_iter_range)\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('num_iterations')\n",
    "plt.title('hyperparameter grid search')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('nDCG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_test_scores = np.mean(test_scores, axis=1)\n",
    "'''\n",
    "plt.figure()\n",
    "plt.plot(parameter_range, mean_test_scores, marker='*', linestyle='--')\n",
    "plt.xlabel(x_str)\n",
    "plt.ylabel('nDCG')\n",
    "plt.title('Num_iterations vs. validation set scores')\n",
    "plt.grid(color='gray',alpha=0.3, linestyle='--')\n",
    "#plt.xticks(np.arange(min(parameter_range), max(parameter_range)+100,))\n",
    "plt.savefig(\"output/num_iterations_vs_ndcg_test.pdf\", bbox_inches='tight')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If results are good save the models into file\n",
    "for model in models:\n",
    "    name_str = 'lgmb_models/model' + str(i*k)+'.txt'\n",
    "    lgbm_model.save_model(name_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ind = -1\n",
    "lgb.plot_importance(models[model_ind], height=1, figsize=(9,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_importances = models[model_ind].feature_importance()/(models[model_ind].current_iteration())\n",
    "feature_importances_normalized = feature_importances/sum(feature_importances)\n",
    "feature_names_importances = dict(zip(models[model_ind].feature_name(),feature_importances_normalized))\n",
    "feature_names_importances = sorted(feature_names_importances.items(), key=lambda x: x[1], reverse=True)\n",
    "feature_names_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
